# Build llama.cpp from source optimized for AMD 8945HS + 780M iGPU
FROM ubuntu:24.04

# Set environment variables for build optimization
ENV DEBIAN_FRONTEND=noninteractive
ENV LLAMA_VULKAN=1
ENV LLAMA_VULKAN_FORCE=1
ENV VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/radeon_icd.x86_64.json
ENV VULKAN_DEVICE=0

# Install build dependencies and Vulkan support
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    pkg-config \
    curl \
    python3 \
    python3-pip \
    python3-dev \
    libvulkan-dev \
    vulkan-tools \
    mesa-vulkan-drivers \
    libvulkan1 \
    libshaderc-dev \
    glslang-tools \
    ninja-build \
    ccache \
    && rm -rf /var/lib/apt/lists/*

# Set up ccache for faster rebuilds
ENV PATH="/usr/lib/ccache:$PATH"
ENV CCACHE_DIR=/tmp/.ccache
RUN mkdir -p /tmp/.ccache

# Clone llama.cpp repository
WORKDIR /usr/src
RUN git clone https://github.com/ggerganov/llama.cpp.git
WORKDIR /usr/src/llama.cpp

# Configure build with AMD 780M iGPU optimizations
RUN cmake -B build \
    -DCMAKE_BUILD_TYPE=Release \
    -DGGML_VULKAN=ON \
    -DGGML_NATIVE=ON \
    -DLLAMA_CURL=ON \
    -DLLAMA_BUILD_TESTS=OFF \
    -DLLAMA_BUILD_EXAMPLES=ON \
    -DLLAMA_BUILD_SERVER=ON \
    -DCMAKE_C_FLAGS="-O3 -march=znver4 -mtune=znver4 -mavx2 -mfma -mf16c" \
    -DCMAKE_CXX_FLAGS="-O3 -march=znver4 -mtune=znver4 -mavx2 -mfma -mf16c" \
    -G Ninja

# Build llama.cpp with all CPU cores
RUN ninja -C build -j$(nproc)

# Create app directory and copy built binaries
WORKDIR /app
RUN cp /usr/src/llama.cpp/build/bin/llama-server /app/ && \
    cp /usr/src/llama.cpp/build/libggml*.so /app/ 2>/dev/null || true

# Install Python dependencies
RUN pip3 install --break-system-packages \
    fastapi==0.115.12 \
    uvicorn==0.34.2 \
    pydantic==2.11.5 \
    numpy==2.2.6 \
    requests==2.32.3 \
    aiofiles==24.1.0

# Copy application files
COPY llama_server_wrapper.py /app/
COPY models/ /app/models/

# Set library path for shared libraries
ENV LD_LIBRARY_PATH="/app:$LD_LIBRARY_PATH"

# Verify Vulkan support
RUN vulkaninfo --summary | head -20 || echo "Vulkan info not available in build environment"

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run the Python wrapper that manages multiple llama-server instances
ENTRYPOINT ["python3", "/app/llama_server_wrapper.py"]